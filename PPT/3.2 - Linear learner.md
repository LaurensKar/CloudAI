# Linear learner

The linear learner algorithm is an algorithm of which very little information can be found. It's can only be used in Amazon SageMaker, and onn [this](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_how-it-works.html) website we read that:

> With the linear learner algorithm, you train with a distributed implementation of stochastic gradient descent (SGD). You can control the optimization process by choosing the optimization algorithm. For example, you can choose to use Adam, AdaGrad, stochastic gradient descent, or other optimization algorithms. You also specify their hyperparameters, such as momentum, learning rate, and the learning rate schedule.

A stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate. ([link](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))

Summarized, it means that we are doing XGBoost, but estimating the gradients in stead of calculating them. Furthermore, AWS uses a distributed implementation, meaning it's no longer calculated on one system, but on many, thereby leveraging the power of AWS.

It can do classification (binary and multiclass) and regression.

Need more info? Check out [https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html), especially the sample notebooks.
