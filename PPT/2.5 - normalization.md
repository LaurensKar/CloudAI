# Scaling or normalization

Normalization is a preprocessing technique used in machine learning to standardize the features or variables of a dataset. It involves transforming the data so that it follows a common scale or range. The purpose of normalization is to bring all features into a similar range, preventing one feature from dominating the learning algorithm over another.

There's an example of this in the wine-exercise.

Normalization is particularly useful when dealing with features that have different scales or units of measurement. By normalizing the data, you ensure that each feature contributes equally to the analysis and model training process. Here are a few commonly used normalization techniques:

1. Min-Max Scaling (Normalization):
   This method scales the data to a fixed range, usually between 0 and 1. It is done by subtracting the minimum value of the feature and dividing it by the range (maximum value minus minimum value) of the feature. The formula is as follows:
   
   ```
   X_normalized = (X - X_min) / (X_max - X_min)
   ```
   
   This method preserves the relative relationships between data points but compresses the range.

2. Z-Score (Standardization):
   Z-score normalization transforms the data to have a mean of 0 and a standard deviation of 1. It is achieved by subtracting the mean of the feature and dividing it by the standard deviation. The formula is as follows:
   
   ```
   X_standardized = (X - X_mean) / X_std
   ```
   
   This method helps in dealing with outliers and works well when the data is normally distributed.

3. Decimal Scaling:
   In decimal scaling, the data is divided by a factor of 10 raised to the power of the number of digits in the maximum absolute value. This brings the values within the range of [-1, 1] or [0, 1] while preserving the order of magnitude. For example, if the maximum absolute value in the feature is 573, the data is divided by 1000, resulting in values ranging from -0.573 to 0.573.
   
These are just a few examples of normalization techniques. The choice of normalization method depends on the characteristics of the dataset and the requirements of the machine learning algorithm being used. It's important to note that normalization is typically applied to the features or independent variables of the dataset and not to the target variable (dependent variable).